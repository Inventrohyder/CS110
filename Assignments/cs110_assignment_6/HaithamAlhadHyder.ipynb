{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism detection\n",
    "\n",
    "**Plagiarism** is the use of another's work and ideas as if they are one's original work. For many institutions, it is a habit that is highly discouraged. In the case of an educational institution, we would want to identify cases of plagiarism so that students can receive a stick for such a habit and thereby discouraging them. However, being in the realm of natural language (with changing rules), the detection of plagiarism is a much more formidable problem than it seems at first. Therefore, in this report, we will look at a method that implements a machine learning algorithm so that as the data of users evolve, so will the detection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String matching\n",
    "\n",
    "If we have two student submissions, _\" today is Monday\"_ and _\"day\"_, concatenating all the words and converting it to lower case, we can point out common substrings of length k (_k-gram_). In this case, if `k` is 3, then the possible (i, j) positions of matches in the first and second strings are (2, 0) and (10, 0). \n",
    "\n",
    "**NOTE:** We do not have to get rid of the spaces but to make our solution much more straightforward, we need to have all the strings in lower case.\n",
    "\n",
    "The following forms the basis of the **containment** feature that compares the occurrences of _k-grams_ in a submitted and source text, relative to the traits of the answer text. The formula for containment is: $$\\frac{\\sum count(\\text{k-gram}_{A}) \\cap count(\\text{k-gram}_{s}) }{\\sum count(\\text{k-gram}_{A}) }$$\n",
    "\n",
    "There are two methods that we will explore that enables us to calculate such matches:\n",
    "1. Rolling hashing\n",
    "2. Regular hash matching\n",
    "\n",
    "> **Citation:** Thomas H. Cormen. “Introduction to Algorithms”. Apple Books. https://books.apple.com/us/book/introduction-to-algorithms/id570172300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rolling hashing\n",
    "\n",
    "To use the rolling hashing technique, we will implement the _Rabin Karp_ algorithm. It has a matching time complexity of $O((n-m+1)m)$, whereby $n$ is the length of the answer text and $m$ is the length of the source text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 12]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helpers\n",
    "\n",
    "# Example usage of Rabin Karp\n",
    "source = \"today is monday\"\n",
    "answer = \"day\"\n",
    "\n",
    "helpers.rabin_karp_matcher(\n",
    "    target=source,\n",
    "    potential=answer,\n",
    "    d=7,\n",
    "    q=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above method is the positions, the potential text appears in the target text. As we can see _\"day\"_ appears in the source text at indexes 2 and 12 (2 occurrences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular hash matching\n",
    "\n",
    "When implementing the regular hash matching algorithm, instead of using previously computed hash values, we calculate all of them from scratch. The time complexity is similar to that of the rolling hashing algorithm, $O((n-m+1)m)$, however, the constants to the time complexity are higher in the regular hash matching algorithm since we compute full hashes each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 12]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.non_rolling_matcher(\n",
    "    target=source,\n",
    "    potential=answer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a similar answer to that of the rolling hashing algorithm. The regular hash matching function hashes the values using the `hash` method in `mmh3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest Common Subsequence\n",
    "\n",
    "The other feature that is crucial in helping us figure out plagiarized works is the longest common subsequence. The LCS of 2 sequences is a sequence of a maximal length that is common between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.longest_common_subsequence(\n",
    "    x=source,\n",
    "    y=answer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "The data  we use for modeling a plagiarism detecting machine learning model is a slightly modified version of a dataset created by Paul Clough (Information Studies) and Mark Stevenson (Computer Science), at the University of Sheffield. Check more information at [their university webpage](https://ir.shef.ac.uk/cloughie/resources/plagiarism_corpus.html).\n",
    "\n",
    "> **Citation for data:** Clough, P. and Stevenson, M. Developing A Corpus of Plagiarized Short Answers, Language Resources and Evaluation: Special Issue on Plagiarism and Authorship Analysis, In Press. [Download]\n",
    "\n",
    "The data contains _answer_ and _source text files_. The answer text files are based on different task (labeled A-E).\n",
    "\n",
    "Also, the text files, have different levels of plagiarism: \n",
    "1. cut: Copy-pasted directly.\n",
    "2. light: Includes some copying and paraphrasing.\n",
    "3. heavy: Expressed using different words and structure.\n",
    "4. non: Not plagiarized\n",
    "5. orig: The original (source text).\n",
    "\n",
    "> So, out of the submitted files, the only category that does not contain any plagiarism is non.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "> **Citation:** Done in the [feature engineering notebook](./feature_engineering.ipynb)\n",
    "\n",
    "> **Citation:** Since this project is an extension of one of my earlier projects some of the feature engineering code is similar to the one I used in: https://github.com/Inventrohyder/ML_SageMaker_Studies/tree/master/Project_Plagiarism_Detection\n",
    "\n",
    "In this stage we carry out the following tasks:\n",
    "\n",
    "1. we switched the labels of the different files from strings to numbers since it is easier for machine learning algorithms to work with numbers.\n",
    "\n",
    "2. To make it easier, we also split the data into binary (0 for not plagiarized; 1 for plagiarized).\n",
    "\n",
    "4. Process the text to be in lower case only\n",
    "\n",
    "5. Label the data into 'train', 'test' and 'orig'\n",
    "\n",
    "6. Calculate out custom containment value using rolling hashing for different values of `k`\n",
    "\n",
    "7. Calculate longest common subsequence\n",
    "\n",
    "8. To avoid correlated features that would be redundant, only pick the columns that have a correlation lower than 0.95\n",
    "\n",
    "![](images/correlation.png)\n",
    "\n",
    "9. Split the data into train and test data files with the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate models\n",
    "\n",
    "> **Citation:** Done in the [train model notebook](train_model.ipynb)\n",
    "\n",
    "To ensure, we implement design thinking, we use different Machine Learning algorithms to fit our data.\n",
    "\n",
    "After evaluation, we get the following results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBF SVM</th>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Process</th>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Net</th>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   scores\n",
       "Nearest Neighbors    0.88\n",
       "Linear SVM           0.92\n",
       "RBF SVM              0.60\n",
       "Gaussian Process     0.56\n",
       "Decision Tree        0.92\n",
       "Random Forest        0.92\n",
       "Neural Net           0.84\n",
       "AdaBoost             0.88\n",
       "Naive Bayes          0.88\n",
       "QDA                  0.84"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('results.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the three models that do really well are _linear SVM_, _Decision Tree_, and _Random Forest_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-centered design\n",
    "\n",
    "Now that we have a model, we can design an application that asks the user for an answer text and a source text, then we calculate all the features we have used to generate the model and we give a prediction of how likely it is that it is plagiarized.\n",
    "\n",
    "To check if the work has been plagiarized from other students work, then we just replace the other student's work as the source text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A: CS110 Dashboard\n",
    "\n",
    "![\"CS110 Dashboard\"](images/my_dashboard_apr29.png \"CS110 Dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B - Link to repository\n",
    "\n",
    "The link to the living repository of the following assignment is: https://github.com/Inventrohyder/CS110/tree/master/Assignments/cs110_assignment_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B - HCs\n",
    "\n",
    "* #designthinking: Building with constant testing and iteration was carried out at each stage to ensure that we build a program that forms the basis of an excellent user experience\n",
    "\n",
    "* #regression: We build the machine learning models using regression and other techniques\n",
    "\n",
    "* #correlation: To pick the right features to use, we use correlation as a metric to evaluate the best ones so that they are not redundant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
